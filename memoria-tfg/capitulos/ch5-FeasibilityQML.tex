% !TeX root = ../tfg.tex
% !TeX encoding = utf8

\chapter{Feasibility of Quantum Machine Learning problems}\label{ch:5- FeasibilityQML}

The target question, specifically on the economic and business point of view, in the field of quantum machine learning is: \textbf{What difference can quantum computers actually make for machine learning?}

The potential power of quantum machine learning algorithms would be a decisive argument to justify the development, and therefore, the investment on quantum computing technologies. Machine learning stands at the forefront of state-of-the-art technology. With the availability of vast amounts of data, powerful computational resources, and advancements in algorithms, machine learning is transforming various industries. After this incentives, researchers hunt for impressive advantages of adding ``quantum'' into the already well established field of machine learning.

The commercial expectations frequently come into conflict with the complex reality of scientific research, where highly complex questions rarely yield straightforward yes-or-no answers. 

Firstly, it is not possible to compare in general quantum machine learning and classical machine learning. It would be like comparing a car and a bike, while they both are means of transportation if we only consider their speed, the comparison may favor the car but if we focus on factors like enviromental impact, cost or ease of maneuverability in traffic the comparison could favor the bike.    To be able to talk about the advantage of quantum machine learning regarding classical machine learning, we would need to fix some parameters: precise what kind of quantum computer we are considering- NISQ, ideal and error corrected-, the task in machine learning- supervised, unsupervised-, the data- size, density-,  and understand what we mean by advantage, which is not simply a question of speed. According to \cite{schuld2021machine}, we can identify four important markers on quantum advantage:

\begin{enumerate}
    \item \textbf{Performance. } The algorithm effectively addresses a learning task, demonstrating its ability to generalize from data samples to previously unseen data.

    \item \textbf{Speedup. } The algorithm shows a faster execution compared to classical competitors that could be used in practice.

    \item  \textbf{Relevance. } The algorithm solves a problem that is relevant to machine learning.

    \item \textbf{Availability. } There is a reasonable likelihood that the technology required to implement the algorithm will become available in the near future.
\end{enumerate}

All four criteria are important in a way to answer the question on how could quantum computing revolutionize machine learning, since there cannot be a true advantage if we match one criteria but not the others. Consider a quantum algorithm with a remarkable generalisation performance but that does not provide any speedup to the classical methods, then we do not need a quantum computer, it provided us a new better classical algorithm. Instead, if there is a speedup, but only for problems of pathological relevance, thus it does not provide much. Finally, considering an algorithm that exceeds in its generalisation performance, execution speed for a relevant problem but only for a type of computer that may be available in a distant future, it is more likely that in the meantime machine learning would progress to a point that it would be needed to reevaluate in the future the advantage of the algorithm against a more advanced machine learning status. 

Currently there is no quantum machine learning algorithm that fulfills all four of this requirements successfully. 

Next, we will discuss more especifically on some matters we have previously mentioned.

\section{Generalisation performance of quantum models}

The generalisation performance, or also known as generalisation power, of a machine learning algorithm refers to its ability to learn from a limited set of data samples and be able to generalise it to previously unseen data. Generalisation requires a machine learning model to be expressive enough to learn a pattern without overfitting. A good model is able to minimise the training error while having the smallest possible expressivity (complexity of functions that can possibly be computed by a parametric function such as a neural net). Then, the expressivity is a crucial factor on the generalisation performance. It is not just a property of the model, it needs to take into account the training procedure. 

In the case of kernel theory, the expressivity of a quantum model is determined by the data encoding feature map, which induces a quantum kernel $\kappa$ that is used to measure distances between data points. Remembering the relationships studied in \autoref{sec:QSVM} between feature maps, kernels and reproducing kernel Hilbert, we can deduce that the espressivity directly depends on the size of the reproducing kernel Hilbert space, formed by the linear combinations of functions $\left\lbrace \kappa(x, \cdot) \right\rbrace$ centred in all points $x \in \mathcal{X}$ of the input domain.

The quantum support vector machine obtained using basis encoding, which maps a $n$ bit binary string $x = (b_1,...b_n) $ with $b_i \in \left\lbrace 0,1\right\rbrace, \quad i=1,...,n$ to a computational basis state, gives us an example of overy expressive kernel. This data encoding maps data to the corners of a hypercube in $\mathbb{C}^{2^n}$. Two classes of data always become linearly separable, but the inner-product between all data points is the same, thus it does not provide useful statements about unseen data points because they uniformly have a zero distance to the training points. A support vector machine will still be able to learn to classify the training data well, by learning the label of each training data point as a weight $\alpha_i$ in \autoref{eq:representer theorem}. Therefore, the kernel induced by a basis encoding feature map will have zero training error and very large test error. This is shown in the reproducing kernel Hilbert space induced by such kernel: it consists of linear combinations of Kronecker delta functions centred in all points $x \in \mathcal{X}$ of the input set, hence functions that are not smooth, in other words, that can show big variations in $f(x)$ for small variations in $x$. Consequently, it is likely to end up in overfitting. 

The main question about generalisation performance of quantum models is still unresolved: How can quantum models strike a balance between flexibility to achieve a low training error and simplicity to prevent overfitting? Furthermore, how might genuine quantum phenomena, such as coherent superpositions of entire datasets or interference in quantum kernels, enhance generalization for specific tasks? Addressing these inquiries requires a genuinely interdisciplinary research approach that incorporates the insights from theory-driven aspects of quantum mechanics and classical machine learning. It is being researched the implications of adapting quantum machine learning algorithms to open quantum systems, aiming to open pathways enhancing the potential of noise and decoherence (\cite{olivera2023benefits}).

\section{Speedup of quantum machine learning}

There are two interpretations of speedup. On the one hand, there is the speedup whose goal is to take a specific classical machine learning algorithm and implement it faster on a quantum computer. This approach would consider it as a potential or limited speedup. According to this interpretation of speedup, the possible quantum speedups derive directly from known results in quantum computing research. These speedups become notably interesting when the subroutine represents a significant bottleneck in the runtime, as the speedup can lead to an overall improvement in the algorithm's runtime. This is often feasible when we make assumptions about the speed at which data can be loaded into the quantum computer.

On the other hand, the second interpretation of speedup refers to the speedup that quantum computers can provide to the process of learning: provided with a dateset, can a quantum computer help the process of identifying patterns in this data, achieving equivalent performance to classical computing but at a faster pace? For this we need to compare the speed and performance of the quantum algorithm to any classical machine learning algorithm. This approach presents greater challenges. Firstly, we need to know the performance of the quantum model. Secondly, according to the no-free-lunch theorems \cite{noFreeLunch}, no machine learning method can universally excel on every dataset, therefore we need to impose assumptions on the data itself. Consequently, we can only study such speedups under highly controlled conditions.  


\section{The size of the dataset}

As we have seen, feature maps are a common element of quantum support vector machines and quantum neural networks. They tackle the problem of ``translating'' the classical input data into quantum states. In many situations, this stage is the bottleneck of the algorithm since in general, data encoding costs linear time in the data size because every feature has to be addressed. If we think of basis encoding, it requires significant spatial constraints concerning data dimension. If each feature is encoded as a $n$-bit binary sequence, we can encode only $100/n$ features within 100 qubits. 

The bottleneck of data encoding implies that, with the exception of a few specific scenarios, quantum machine learning is unlikely to provide practical solutions for processing big data in the near future.  It is needed to adopt solutions to deal with high-dimensional data. However, it's worth noting that certain algorithms, particularly hybrid training approaches, can overcome this limitation by processing a subset of samples from the dataset at a time, which alleviates the constraints on the number of data samples.

The promise of quantum techniques for handling big data sound appealing but the challenges are indeed daunting. There is an undeniable global enthusiasm surrounding big data, nonetheless, there are numerous scenarios where data collection is costly or naturally constrained; hence, predictors have to be built for extremely small datasets. If quantum computing can demonstrate a qualitative advantage, it will undoubtedly find valuable applications in the realm of ``small data''.

\section{The best of both worlds}

Hybrid schemes in which only a small part of the model is processed by a quantum device, align better with intermediate-term technologies. Hybrid quantum algorithms offer the advantage of employing quantum devices for relatively short tasks, alternating with classical computations. Another significant advantage of hybrid methods is that their parameters are accessible as classical data, making them straightforward to store and use for predicting multiple inputs.

Amongst hybrid algorithms, variational circuits are particularly promising: the combination of quantum neural networks with classical neural networks, since they are two models that naturally fit together. The general approach involves incorporating a quantum neural network as a layer within a conventional classical neural network. In this way, the “quantum layer” will take as input the outputs of the previous layer (or the inputs to the model, if there’s no layer before it) and will forward its output to the next layer (if there are any). The output of the quantum neural network will be a numerical array of length $n$; thus, in the eyes of the next layer, the quantum layer will behave as if it were a classical layer with $n$ neurons.

Variational algorithms in the context of machine learning open a rich research area, with the prospect of developing an entirely new field within classical machine learning.


\section{Future perspectives}

Quantum computing is a field with strong roots in theory. While the mathematical foundations were essentially established in the 1930s, we continue to explore their practical applications up to today. Algorithmic development in quantum computing often relies on theoretical proofs to demonstrate the potential of quantum algorithms, especially when hardware constraints make numerical arguments difficult to employ. On the other hand, machine learning methods are largely of practical nature and breakthroughs are often the result of huge numerical experiments. The merits of machine learning range from its computational complexity to the generalisation power of a model, its ease of use, mathematical and algorithmic
simplicity and wide applicability. 

When trying to put together these two fields, we suffer some limitations due to the reduced scale and resilience to the noise of current quantum computers, which in turn affects possible results we can obtain executing quantum machine learning algorithms on real quantum hardware. The goal of demostrating superior performance of methods that—at least at this stage—we only have limited practical access to is challenging. We have to take into account that quantum machine learning is a relatively young research discipline,  although it has already come a long way and we are not pessimistic. Scientist will continue researching and expanding the boundaries of our understanding, exploring new techniques for processing information with quantum devices. 

Our discussion so far has been oriented on what can quantum computing provide to machine learning. However, quantum machine learning may gain perspective by shifting the roles and asking what machine learning can offer to quantum computing. As we had mentioned before, machine learning can be the ''greatest application'' that makes quantum computing commercially viable by  connecting the emerging fields of quantum computing with an already established wealthy market. But beyond the economic interests, perhaps machine learning can inspire quantum computing in NISQ era to add methods to its ``toolbox'', looking for more practical methods even if they are less rigorous. Machine learning is a field of computer science that revolves about the idea of what it means to learn. Quantum machine learning extends the idea of learning into the realm of quantum information processing, raising numerous abstract questions that aim our knowledge and research rather than finding commercial applications.

Limiting the scope of quantum machine learning may help obtaining better results, more solid and powerful models and addressing some matters such as what makes a good data embedding or a good trainable variational form. Models use designs where circuits inherit traditional quantum gates whose goal is to enhance the expressive power. However, this approach may be more driven by our limited understanding rather than any other fundamental reason. In classical machine learning, neural networks do not rely on arbitrary structures, but are based on fundamental mechanisms derived form biological processes. If we take into account Gaussian kernels, they rise from the natural distance measures of Gaussian functions. Although both can be used to asymptotically approximate any function, they are based on specific computations, far away from being an ansatz for ``general classical computations''. To succeed in building powerful models from quantum circuits we need to find a basic structure of quantum models equivalent to what the perceptron and the Gaussian are to classical machine learning.

The restriction of the scope of quantum machine learning should also be applied to the problems, datasets and applications that it is targeted to. Most quantum computing researchers agree that it is unlikely that quantum computers will supersede classical computers as a whole, instead, quantum computers could turn out to be convenient accelerators or special-purpose hardware devices for very specific applications.

Optimistically quantum machine learning may be one of the first applications for quantum computers, but pessimistically, it may also just remain as a research discipline because machine learning on classical computers has already an impressive performance. Quantum machine learning is not just a collection of applications, but a new exciting, challenging and intriguing approach to learning from data when it is processed according to the laws of quantum mechanics, which draws a relevant connection between the quantum nature of the world and the abstract concept of learning. In any case, the possibilities offered by quantum computing deserve the intense research activity that is currently carried on. It is important leaving aside sensationalist claims on quantum computing and being realistic with the possibilities of it at its current near-term noisy state.


\endinput
%--------------------------------------------------------------------
% FIN DEL CAPÍTULO. 
%--------------------------------------------------------------------
